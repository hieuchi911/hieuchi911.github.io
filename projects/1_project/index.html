<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Distillation mitigates Hallucinations | Hieu Tran-Chi Nguyen </title> <meta name="author" content="Hieu Tran-Chi Nguyen"> <meta name="description" content="On the effects of Knowledge Distillation on LLM Hallucinations (ongoing)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, hieu-nguyen-ortfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%E2%80%8D%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hieuchi911.github.io/projects/1_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Hieu Tran-Chi Nguyen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Distillation mitigates Hallucinations</h1> <p class="post-description">On the effects of Knowledge Distillation on LLM Hallucinations (ongoing)</p> </header> <article> <h3><b>Exciting news as of 02/2025</b></h3> <p>üéâ I‚Äôm excited to announce that my first-author research, ‚ÄúSmoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation‚Äù, is now available as a <a href="https://arxiv.org/abs/2502.11306" rel="external nofollow noopener" target="_blank">preprint</a>!</p> <p>üßê In this work, we found that hard labels in cross entropy loss carry arbitrary assumptions, leading models to make assumptions when processing information and hallucinate. We used knowledge distillation (KD) to remedy this by using teacher-generated distributions as contextual soft labels. Our experiments showed decent improvements of KD models over SFT baselines in common hallucination benchmarks across internal and external metrics!</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_hallu_cnndm-480.webp 480w,/assets/img/work1_hallu_cnndm-800.webp 800w,/assets/img/work1_hallu_cnndm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_hallu_cnndm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="External metrics performance of SFT and KD of Llama-2 and Llama-3.1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_hallu_cnndm-480.webp 480w,/assets/img/work1_hallu_cnndm-800.webp 800w,/assets/img/work1_hallu_cnndm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_hallu_cnndm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="External metrics performance of SFT and KD of Llama-2 and Llama-3.1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Hallucination evaluation results for student models finetuned with supervised finetuning (SFT) and knowledge distillation (KD). Models are evaluated on the CNN/Daily Mail (CNNDM) and XSUM datasets using three metrics: ROUGE-L (‚Üë, %) for n-gram overlap, factual consistency (‚Üë, %) for context grounding, and factual rate (‚Üë, %) for specialized hallucination detection.The results suggest that in most cases KD reduces hallucination compared to SFT, as models trained with soft labels from a teacher model demonstrate improved faithfulness. </div> <p>I would like to express my deepest gratitude to Dr. <a href="https://zihaohe123.github.io/" rel="external nofollow noopener" target="_blank">Zihao He</a> for his guidance so far throughout this research. I would also like to thank <a href="https://isi.edu/" rel="external nofollow noopener" target="_blank">USC ISI</a> for providing the hardware infrastructure that made these experiments possible!</p> <h3><b>The journey</b></h3> <h4><b>Motivation</b></h4> <p>In my senior year of masters, I conducted an analysis on the optimization aspect in traditional training of decoder transformers with One Hot Encoding (OHE) target distributions and found that: <b><i>by optimizing models against OHE targets of zero entropy, we train them to make assumptions</i></b> (<a href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy#:~:text=To%20choose%20a%20distribution%20with%20lower%20entropy%20would%20be%20to%20assume%20information%20we%20do%20not%20possess" rel="external nofollow noopener" target="_blank">principle of Maximum Entropy</a> states that to choose a distribution with entropy lower than allowed by the provided information would be to assume information we do not possess).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_maxentropy-480.webp 480w,/assets/img/work1_maxentropy-800.webp 800w,/assets/img/work1_maxentropy-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_maxentropy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="OHE targets cause hallucinations" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Given the context "The student did well in", the distribution of the next token should have high probability for subjects, sports, arts, etc. However, OHE targets force the model to choose one token with 100% probability to be "Physics", an information not provided in the context. Thus, the model learns to make assumptions and thus hallucinate. </div> <p>I hypothesized that this leads to hallucination and that Sequence and Word-level Knowledge Distillation (KD) in [3], replacing ignorant OHE targets with teacher context-aware distributions, avoids assumptions and helps reduce hallucination:</p> \[\mathcal{L} = (1-\alpha) \mathcal{L}_{NLL}(\theta) + \alpha \mathcal{L}_{KD}(\theta, \theta_T)\] <p>, where \(\mathcal{L}_{NLL}\) is the negative log-likelihood loss, which is the Cross-entropy between student distributions with OHE targets, \(\mathcal{L}_{KD}\) is the Cross-entropy between the teacher and student distributions, and \(\alpha\) is the weight of the KD loss.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_kdhelps-480.webp 480w,/assets/img/work1_kdhelps-800.webp 800w,/assets/img/work1_kdhelps-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_kdhelps.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="OHE targets cause hallucinations" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Replace OHE targets with context-aware distributions (non-zero probability for other possibilities, e.g. Maths, Arts, English) from a teacher model, the model learns to generate tokens that are more contextually relevant and less hallucinatory. </div> <p>I began working with Dr. <a href="https://zihaohe123.github.io/" rel="external nofollow noopener" target="_blank">Zihao He</a> at <a href="https://lermanlab.github.io/" rel="external nofollow noopener" target="_blank">SEA (Socially-Embedded AI) Lab</a> led by Professor <a href="https://www.isi.edu/people/lerman/about" rel="external nofollow noopener" target="_blank">Kristina Lerman</a> to study the effects of KD on LLM Hallucinations.</p> <h4><b>Experiments</b></h4> <p>To verify my hypothesis, I instruction finetuned Llama-2-7B and Llama-3.1-8B pretrained LLMs under two methods: traditional <b>Supervised Fine-Tuning (SFT)</b> as baselines and <b>KD</b> (with 13B and 70B teachers, respectively), and then compare them using a robust hallucination evaluation pipeline. For KD, following the recipe in [3], I first ran SFT of teacher models, and then use these teachers to augment the training dataset with beam-search, which will then be used to train student models. Advised by Cohere For AI researchers, I evaluated models with benchmarks of summarization tasks, using external metrics (rougeL, <a href="https://vectara.com/blog/hhem-v2-a-new-and-improved-factual-consistency-scoring-model/%20$$\mathcal{fc}$$" rel="external nofollow noopener" target="_blank">factual consistency</a> from Vectara) and a non-conventional internal metric (factual rate \(\mathcal{fr}\) from Lookback-Lens in [2]).</p> <h4><b>Results</b></h4> <p>Results from ablation experiments with search space \(lr \in \{1e-05, 5e-06\}\) and \(bs \in \{4, 8\}\) show that for both Llama-2 and Llama3.1, KD models with \(\alpha=0.1\) consistently outperform SFT baselines for ROUGE-L on both CNN-DM and XSUM benchmark. </p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_hallu_cnndm-480.webp 480w,/assets/img/work1_hallu_cnndm-800.webp 800w,/assets/img/work1_hallu_cnndm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_hallu_cnndm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="External metrics performance of SFT and KD of Llama-2 and Llama-3.1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_hallu_cnndm-480.webp 480w,/assets/img/work1_hallu_cnndm-800.webp 800w,/assets/img/work1_hallu_cnndm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_hallu_cnndm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="External metrics performance of SFT and KD of Llama-2 and Llama-3.1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> RougeL on CNN-DM and XSUM hallucination benchmarks from ablation experiments of Llama-2 and Llama-3.1 (the higher the better): SFT baselines (<i>sft</i>) and KD models with <b>Œ±</b>; of 0.1 (<i>kd0.1</i>), 1.0 (<i>kd1.0</i>), 10.0 (<i>kd10.0</i>), 0.002 (<i>kd0.002</i>), 0.05 (<i>kd0.05</i>). Entries are from models trained with <b>lr ‚àà {1e-05, 5e-06}</b> and <b>bs ‚àà {4, 8}</b>. </div> <p>Results on the internal mertric of factual rate also shows improvements of KD over SFT Llama-2 on CNN-DM. XSUM, however, reflects no such improvements.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_llama_2_internal-480.webp 480w,/assets/img/work1_llama_2_internal-800.webp 800w,/assets/img/work1_llama_2_internal-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_llama_2_internal.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> </div> <p>We also looked at the performance of pretrained models, and observed that finetuning, though did not necessarily yield better performance on downstream benchmarks, but under KD seems to yield reduced hallucination in comparison with SFT.</p> <p>Below is the boxplots view of the results:</p> <ul> <li>Llama-2 SFT and KDs:</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_external_metrics_rougeL-480.webp 480w,/assets/img/work1_external_metrics_rougeL-800.webp 800w,/assets/img/work1_external_metrics_rougeL-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_external_metrics_rougeL.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="RougeL Llama-2" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_external_metrics_fs-480.webp 480w,/assets/img/work1_external_metrics_fs-800.webp 800w,/assets/img/work1_external_metrics_fs-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_external_metrics_fs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Factual consistency Llama-2" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_internal_metrics_lookback_ratios-480.webp 480w,/assets/img/work1_internal_metrics_lookback_ratios-800.webp 800w,/assets/img/work1_internal_metrics_lookback_ratios-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_internal_metrics_lookback_ratios.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Factual rate Llama-2" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Boxplots of ROUGE-L (left), Factual consistency (middle), and Factual rate (right) from ablation experiments (the higher the better): SFT Llama-2-7B baselines (<i>sft</i>) and KD Llama-2-7B with <b>Œ±</b>; of 0.1 (<i>kd01</i>), 1.0 (<i>kd1</i>), 10.0 (<i>kd10</i>); <i>KD</i> is the aggregate performance across all <b>Œ±</b>; values. Boxplots are from entries of models trained with <b>lr ‚àà {1e-05, 5e-06}</b> and <b>bs ‚àà {4, 8}</b>. </div> <ul> <li>Llama-3.1 SFT and KDs:</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_external_metrics_rougeL-llama3.1-480.webp 480w,/assets/img/work1_external_metrics_rougeL-llama3.1-800.webp 800w,/assets/img/work1_external_metrics_rougeL-llama3.1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_external_metrics_rougeL-llama3.1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="RougeL Llama-3.1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_external_metrics_fs-llama3.1-480.webp 480w,/assets/img/work1_external_metrics_fs-llama3.1-800.webp 800w,/assets/img/work1_external_metrics_fs-llama3.1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_external_metrics_fs-llama3.1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Factual consistency Llama-3.1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Boxplots of ROUGE-L (left) and Factual consistency (right) from ablation experiments (the higher the better): SFT Llama-3.1-8B baselines (<i>sft</i>) and KD Llama-3.1-8B with <b>Œ±</b>; of 0.1 (<i>kd01</i>), 1.0 (<i>kd1</i>), 10.0 (<i>kd10</i>); <i>KD</i> is the aggregate performance across all <b>Œ±</b>; values. Boxplots are from entries of models trained with <b>lr ‚àà {1e-05, 5e-06}</b> and <b>bs ‚àà {2, 4}</b>. </div> <p>For both XSUM and CNN-DM are abstractive summarization benchmarks, these results show that:</p> <ul> <li>XSUM is more difficult than CNN-DM. Improvements on CNN-DM do not fully indicate superiority in avoiding hallucination.</li> <li>Degradation of Llama-3.1 on XSUM might be due to the degradation in its performance in general, as shown in its worse performance on all benchmarks comparing to the pretrained.</li> <li>Factual consistency does not fully correlate with LLM hallucinations (pointed out <a href="https://github.com/vectara/hallucination-leaderboard#:~:text=Wouldn%27t%20an%20extractive%20summarizer%20model%20that%20just%20copies%20and%20pastes%20from%20the%20original%20summary%20score%20100%25" rel="external nofollow noopener" target="_blank">here</a>).</li> </ul> <p>Prior to this, we also got similar improvements on hallucination benchmarking for models of size 300M-1B:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_res_sLLM_QA-480.webp 480w,/assets/img/work1_res_sLLM_QA-800.webp 800w,/assets/img/work1_res_sLLM_QA-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_res_sLLM_QA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Model (fine-tuned with Llama as the teacher on the PubMedQA dataset). Evaluation Results on Truthful QA and Hotpot QA for Q&amp;A task </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/work1_res_sLLM_summ-480.webp 480w,/assets/img/work1_res_sLLM_summ-800.webp 800w,/assets/img/work1_res_sLLM_summ-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/work1_res_sLLM_summ.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Model (fine-tuned with Mistral as the teacher on the DialogSum dataset). Evaluation Results on CNN Dailymail for summarization task </div> <h4><b>Conclusions</b></h4> <p>These results partly consolidated my hypothesis, as well as the trustworthiness of mainstream metrics by its consistency with the novel internal metrics.</p> <p>In abstractive summarization XSUM, while ROUGE-L indicates that KD models outperform SFT models, factual rate shows that KD models are just as good as the baselines. This can be accounted by two factors: either factual rate or ROUGE-L does not perform well estimating data under XSUM. However, factual rate was reported to generalize to XSUM even though it was trained from attention weights under CNN-DM. Thus, this shows that ROUGE-L may overestimate models‚Äô performance, giving false conclusions on their hallucination ability, and factual rate can uncover these subtle insights overlooked by ROUGE-L. This insight also highlights the importance of diverse evaluation pipeline.</p> <p>As we progress, we want to verify the hypothesis with more model families like Mistral, Gemma, and Qwen.</p> <h4><b>Future works</b></h4> <p>There are many questions that I want to explore next:</p> <ul> <li>[1] has shown that models struggle to answer closed-book real-world questions even when they have learned the relevant details. This indicates that such behavior may be intrinsic to the models themselves. Thus, it is likely that even when provided with context, they will still hallucinate. Indeed, hallucination persists even in RAG settings. This raises a significant question: <b>is a model‚Äôs tendency to generate inaccurate responses based solely on its own learned internal knowledge (factuality hallucination) the main reason it generates inaccurate responses when given external contexts (faithfulness hallucination)?</b> If so, addressing the issue of factuality hallucination might be the key to resolving the problem. These definitions have also revealed that our work has not focused on factuality hallucinations, which will be our next priority. In fact, insights on both types of hallucinations will shed light on the previously raised question.</li> <li>Are acquired improvements on external metrics were merely due to better task fulfillment instead of hallucination-free reasoning? This is because these metrics only compare prediction text to ground truth text, while hallucination can be subtle and requires non-trivial reasoning to identify. In fact, we‚Äôve already started tackling this question with internal metrics and proved the effectiveness of external metrics.</li> <li>Other factors causing hallucination: exposure bias, data imbalance, attend-to-all mechanism that distract models, etc.</li> </ul> <h4><b>References</b></h4> <p>[1] - Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu <a href="https://dl.acm.org/doi/abs/10.1145/3703155" rel="external nofollow noopener" target="_blank">A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions</a>. <i>arXiv:2311.05232</i>, 2023.</p> <p>[2] - Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, James Glass <a href="https://arxiv.org/abs/2407.07071" rel="external nofollow noopener" target="_blank">Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps</a>. <i>arXiv:2407.07071</i>, 2024.</p> <p>[3] - Yoon Kim, Alexander M. Rush <a href="https://arxiv.org/abs/1606.07947" rel="external nofollow noopener" target="_blank">Sequence-Level Knowledge Distillation</a>. <i>arXiv:1606.07947</i>, 2016.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Hieu Tran-Chi Nguyen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 22, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>